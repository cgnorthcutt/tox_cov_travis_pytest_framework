{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Tutorial using `hyperopt`\n",
    "\n",
    "In this simple tutorial, we show how to use hyperopt on the well known Iris dataset from scikit-learn. We use a neural network as the model, but any model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division, unicode_literals, with_statement\n",
    "\n",
    "from hypopt import GridSearch\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Neural Network imports (simple sklearn Neural Network)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Silence neural network SGD convergence warnings.\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [\"constant\", \"adaptive\"],\n",
    "    'hidden_layer_sizes': [(100,20), (500,20), (20,10), (50,20), (4,2)],\n",
    "    'alpha': [0.0001], # minimal effect\n",
    "    'warm_start': [False], # minimal effect\n",
    "    'momentum': [0.1, 0.9], # minimal effect\n",
    "    'learning_rate_init': [0.001, 0.01, 1],\n",
    "    'max_iter': [50],\n",
    "    'random_state': [0],\n",
    "    'activation': ['relu'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set sizes: 73 (train), 32 (val), 45 (test)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris[\"data\"], iris[\"target\"], test_size = 0.3, random_state = 0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train,test_size = 0.3, random_state = 0)\n",
    "print('Set sizes:', len(X_train), '(train),', len(X_val), '(val),', len(X_test), '(test)')\n",
    "\n",
    "# Normalize data \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_val = scaler.transform(X_val)  \n",
    "X_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-search time comparison using validation set versus cross-validation. \n",
    "### The hyperopt package automatically distributes work on all CPU threads regardless of if you use a validation set or cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First let's try the neural network with default parameters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=50, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=0, shuffle=True,\n",
       "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST SCORE (default parameters): 0.8444\n",
      "VALIDATION SCORE (default parameters): 0.9063\n"
     ]
    }
   ],
   "source": [
    "print(\"First let's try the neural network with default parameters.\")\n",
    "default = MLPClassifier(max_iter=50, random_state=0)\n",
    "default.fit(X_train, y_train)\n",
    "test_score = round(default.score(X_test, y_test), 4)\n",
    "val_score = round(default.score(X_val, y_val), 4)\n",
    "print('\\nTEST SCORE (default parameters):', test_score)\n",
    "print('VALIDATION SCORE (default parameters):', val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid-search using a validation set.\n",
      " -------------------------------------------------------------------------------\n",
      "Comparing 60 parameter setting(s) using 4 CPU thread(s) ( 15 job(s) per thread ).\n",
      "CPU times: user 20 ms, sys: 23.9 ms, total: 43.9 ms\n",
      "Wall time: 844 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation=u'relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 20), learning_rate=u'constant',\n",
       "       learning_rate_init=0.01, max_iter=50, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=0, shuffle=True,\n",
       "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST SCORE (hyper-parameter optimization with validation set): 0.9556\n",
      "VALIDATION SCORE (hyper-parameter optimization with validation set): 0.9375\n"
     ]
    }
   ],
   "source": [
    "gs_val = GridSearch(model = MLPClassifier(max_iter=50, random_state=0))\n",
    "print(\"Grid-search using a validation set.\\n\",\"-\"*79)\n",
    "%time gs_val.fit(X_train, y_train, param_grid, X_val, y_val, scoring = 'accuracy')\n",
    "test_score = round(gs_val.score(X_test, y_test), 4)\n",
    "val_score = round(gs_val.score(X_val, y_val), 4)\n",
    "print('\\nTEST SCORE (hyper-parameter optimization with validation set):', test_score)\n",
    "print('VALIDATION SCORE (hyper-parameter optimization with validation set):', val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Let's see how long grid-search takes to run when we don't use a validation set.\n",
      "Grid-search using cross-validation.\n",
      " -------------------------------------------------------------------------------\n",
      "CPU times: user 1.07 s, sys: 76.3 ms, total: 1.15 s\n",
      "Wall time: 4.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation=u'relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 20), learning_rate=u'constant',\n",
       "       learning_rate_init=0.01, max_iter=50, momentum=0.1,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=0, shuffle=True,\n",
       "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST SCORE (hyper-parameter optimization with cross-validation): 0.9556\n",
      "VALIDATION SCORE (hyper-parameter optimization with cross-validation): 0.9375\n",
      "\n",
      "Note that although its slower, cross-validation has many benefits (e.g. uses all\n",
      "your training data). Thats why hyperopt also supports cross-validation when no validation \n",
      "set is provided as in the example above.\n"
     ]
    }
   ],
   "source": [
    "gs_cv = GridSearch(model = MLPClassifier(max_iter=50, random_state=0), cv_folds=6)\n",
    "print(\"\\n\\nLet's see how long grid-search takes to run when we don't use a validation set.\")\n",
    "print(\"Grid-search using cross-validation.\\n\",\"-\"*79)\n",
    "%time gs_cv.fit(X_train, y_train, param_grid)\n",
    "test_score = round(gs_cv.score(X_test, y_test), 4)\n",
    "val_score = round(gs_cv.score(X_val, y_val), 4)\n",
    "print('\\nTEST SCORE (hyper-parameter optimization with cross-validation):', test_score)\n",
    "print('VALIDATION SCORE (hyper-parameter optimization with cross-validation):', val_score)\n",
    "print('''\\nNote that although its slower, cross-validation has many benefits (e.g. uses all\n",
    "your training data). Thats why hyperopt also supports cross-validation when no validation \n",
    "set is provided as in the example above.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can view the best performing parameters and their scores.\n",
      "{u'warm_start': False, u'hidden_layer_sizes': (100, 20), u'activation': u'relu', u'max_iter': 50, u'random_state': 0, u'momentum': 0.9, u'alpha': 0.0001, u'learning_rate': u'adaptive', u'learning_rate_init': 0.01}\n",
      "Score: 0.9375\n",
      "{u'warm_start': False, u'hidden_layer_sizes': (50, 20), u'activation': u'relu', u'max_iter': 50, u'random_state': 0, u'momentum': 0.1, u'alpha': 0.0001, u'learning_rate': u'adaptive', u'learning_rate_init': 0.01}\n",
      "Score: 0.9375\n",
      "\n",
      "Verify that the lowest scoring parameters make sense.\n",
      "{u'warm_start': False, u'hidden_layer_sizes': (4, 2), u'activation': u'relu', u'max_iter': 50, u'random_state': 0, u'momentum': 0.9, u'alpha': 0.0001, u'learning_rate': u'constant', u'learning_rate_init': 1}\n",
      "Score: 0.3125\n",
      "{u'warm_start': False, u'hidden_layer_sizes': (4, 2), u'activation': u'relu', u'max_iter': 50, u'random_state': 0, u'momentum': 0.9, u'alpha': 0.0001, u'learning_rate': u'adaptive', u'learning_rate_init': 1}\n",
      "Score: 0.3125\n",
      "\n",
      "Alas, these did poorly because the hidden layers are too small (4,2) and the learning rate is too high (1).\n",
      "Also, note that some of the scores are the same. With hyperopt, you can view all scores and settings to see which parameters really matter.\n"
     ]
    }
   ],
   "source": [
    "print('We can view the best performing parameters and their scores.')\n",
    "for z in gs_val.get_param_scores()[:2]:\n",
    "    p, s = z\n",
    "    print(p)\n",
    "    print('Score:', s)\n",
    "print()\n",
    "print('Verify that the lowest scoring parameters make sense.')\n",
    "for z in gs_val.get_param_scores()[-2:]:\n",
    "    p, s = z\n",
    "    print(p)\n",
    "    print('Score:', s)\n",
    "print('\\nAlas, these did poorly because the hidden layers are too small (4,2) and the learning rate is too high (1).')\n",
    "print('Also, note that some of the scores are the same. With hyperopt, you can view all scores and settings to see which parameters really matter.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
