{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST example with hyper-parameter optimization on a validation set using `hyperopt`\n",
    "\n",
    "#### In this simple tutorial, we show how to use hyperopt on the well known MNIST handwritten digits dataset. We use a neural network as the model, but any model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division, unicode_literals, with_statement\n",
    "\n",
    "from hypopt import GridSearch\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Neural Network imports (simple sklearn Neural Network)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Silence neural network SGD convergence warnings.\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "# requires pip install mnist\n",
    "import mnist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [\"constant\"],\n",
    "    'hidden_layer_sizes': [(1000,20), (10,5)],\n",
    "    'alpha': [0.0001], # minimal effect\n",
    "    'warm_start': [False], # minimal effect\n",
    "    'momentum': [0.9], # minimal effect\n",
    "    'learning_rate_init': [0.001, 0.005],\n",
    "    'random_state': [0],\n",
    "    'activation': ['relu'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set sizes: 48000 (train), 12000 (val), 10000 (test)\n"
     ]
    }
   ],
   "source": [
    "# Get data - this make take some time to download the first time.\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    mnist.train_images().reshape(60000, 28 * 28).astype(float), \n",
    "    mnist.train_labels(), \n",
    "    stratify=mnist.train_labels(),\n",
    "    test_size = 0.2, \n",
    "    random_state = 0,\n",
    ")\n",
    "\n",
    "X_test, y_test = mnist.test_images().reshape(10000, 28 * 28).astype(float), mnist.test_labels()\n",
    "print('Set sizes:', len(X_train), '(train),', len(X_val), '(val),', len(X_test), '(test)')\n",
    "\n",
    "# Normalize data \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_val = scaler.transform(X_val)  \n",
    "X_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-search time comparison using validation set versus cross-validation. \n",
    "#### The hyperopt package automatically distributes work on all CPU threads regardless of if you use a validation set or cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First let's try the neural network with default parameters.\n",
      "CPU times: user 20.8 s, sys: 796 ms, total: 21.6 s\n",
      "Wall time: 3.59 s\n",
      "\n",
      "TEST SCORE (default parameters): 0.9695\n",
      "VALIDATION SCORE (default parameters): 0.968\n"
     ]
    }
   ],
   "source": [
    "print(\"First let's try the neural network with default parameters.\")\n",
    "default = MLPClassifier(max_iter=6, random_state=0)\n",
    "%time default.fit(X_train, y_train)\n",
    "test_score = round(default.score(X_test, y_test), 4)\n",
    "val_score = round(default.score(X_val, y_val), 4)\n",
    "print('\\nTEST SCORE (default parameters):', test_score)\n",
    "print('VALIDATION SCORE (default parameters):', val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid-search using a validation set.\n",
      " -------------------------------------------------------------------------------\n",
      "Comparing 4 parameter setting(s) using 12 CPU thread(s) ( 1 job(s) per thread ).\n",
      "CPU times: user 2.52 s, sys: 2.31 s, total: 4.83 s\n",
      "Wall time: 37.5 s\n",
      "\n",
      "TEST SCORE (hyper-parameter optimization with validation set): 0.9712\n",
      "VALIDATION SCORE (hyper-parameter optimization with validation set): 0.9717\n"
     ]
    }
   ],
   "source": [
    "gs_val = GridSearch(model = MLPClassifier(max_iter=6, random_state=0))\n",
    "print(\"Grid-search using a validation set.\\n\",\"-\"*79)\n",
    "%time gs_val.fit(X_train, y_train, param_grid, X_val, y_val)\n",
    "test_score = round(gs_val.score(X_test, y_test), 4)\n",
    "val_score = round(gs_val.score(X_val, y_val), 4)\n",
    "print('\\nTEST SCORE (hyper-parameter optimization with validation set):', test_score)\n",
    "print('VALIDATION SCORE (hyper-parameter optimization with validation set):', val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Let's see how long grid-search takes to run when we don't use a validation set.\n",
      "Grid-search using cross-validation.\n",
      " -------------------------------------------------------------------------------\n",
      "CPU times: user 2min 5s, sys: 2.7 s, total: 2min 8s\n",
      "Wall time: 3min 9s\n",
      "\n",
      "TEST SCORE (hyper-parameter optimization with cross-validation): 0.9712\n",
      "VALIDATION SCORE (hyper-parameter optimization with cross-validation): 0.9717\n",
      "\n",
      "Note that although its slower, cross-validation has many benefits (e.g. uses all\n",
      "your training data). Thats why hyperopt also supports cross-validation when no validation \n",
      "set is provided as in the example above.\n"
     ]
    }
   ],
   "source": [
    "gs_cv = GridSearch(model = MLPClassifier(max_iter=6, random_state=0), cv_folds=5)\n",
    "print(\"\\n\\nLet's see how long grid-search takes to run when we don't use a validation set.\")\n",
    "print(\"Grid-search using cross-validation.\\n\",\"-\"*79)\n",
    "%time gs_cv.fit(X_train, y_train, param_grid)\n",
    "test_score = round(gs_cv.score(X_test, y_test), 4)\n",
    "val_score = round(gs_cv.score(X_val, y_val), 4)\n",
    "print('\\nTEST SCORE (hyper-parameter optimization with cross-validation):', test_score)\n",
    "print('VALIDATION SCORE (hyper-parameter optimization with cross-validation):', val_score)\n",
    "print('''\\nNote that although its slower, cross-validation has many benefits (e.g. uses all\n",
    "your training data). Thats why hyperopt also supports cross-validation when no validation \n",
    "set is provided as in the example above.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can view the best performing parameters and their scores.\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (1000, 20), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'momentum': 0.9, 'random_state': 0, 'warm_start': False}\n",
      "Score: 0.9716666666666667\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (1000, 20), 'learning_rate': 'constant', 'learning_rate_init': 0.005, 'momentum': 0.9, 'random_state': 0, 'warm_start': False}\n",
      "Score: 0.96125\n",
      "\n",
      "Verify that the lowest scoring parameters make sense.\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 5), 'learning_rate': 'constant', 'learning_rate_init': 0.005, 'momentum': 0.9, 'random_state': 0, 'warm_start': False}\n",
      "Score: 0.91375\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 5), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'momentum': 0.9, 'random_state': 0, 'warm_start': False}\n",
      "Score: 0.8949166666666667\n",
      "\n",
      "Alas, these did poorly because the hidden layers are too small (10,5).\n"
     ]
    }
   ],
   "source": [
    "print('We can view the best performing parameters and their scores.')\n",
    "for z in gs_val.get_param_scores()[:2]:\n",
    "    p, s = z\n",
    "    print(p)\n",
    "    print('Score:', s)\n",
    "print()\n",
    "print('Verify that the lowest scoring parameters make sense.')\n",
    "for z in gs_val.get_param_scores()[-2:]:\n",
    "    p, s = z\n",
    "    print(p)\n",
    "    print('Score:', s)\n",
    "print('\\nAlas, these did poorly because the hidden layers are too small (10,5).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
